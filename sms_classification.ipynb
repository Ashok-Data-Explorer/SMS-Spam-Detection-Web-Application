{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMB7V7FG3r2x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sms=pd.read_csv(\"/content/Spam_SMS.csv\")"
      ],
      "metadata": {
        "id": "kz0otugq4POR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms.head(5)"
      ],
      "metadata": {
        "id": "HNnWsmTt4VsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms.tail()"
      ],
      "metadata": {
        "id": "IzGAfIcX4Y_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms.shape"
      ],
      "metadata": {
        "id": "zBg3oZoi4cws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms.isnull().sum()"
      ],
      "metadata": {
        "id": "jbyq9I5E4fTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms.duplicated().sum()"
      ],
      "metadata": {
        "id": "coVXJTCr4jaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "niK0WmfG4ouA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms.shape"
      ],
      "metadata": {
        "id": "qkCWCLf14tUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=sms.copy()"
      ],
      "metadata": {
        "id": "Zu_gH61s44yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "Kh527RDz5Mvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Class'] = data['Class'].replace({'ham': 0, 'spam': 1})\n"
      ],
      "metadata": {
        "id": "VyrqvvUQ4vcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "y-eNX9Dy53z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report ,accuracy_score,confusion_matrix\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "RGHzi36356jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "l0R_TYIc7Er4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the stopwords in English\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "XMWSg4zz7Isn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizing(content):\n",
        "    # Remove noise (URLs, mentions, etc.)\n",
        "    content = re.sub(r'http\\S+|www\\S+|https\\S+', '', content, flags=re.MULTILINE)\n",
        "    content = re.sub(r'@\\w+', '', content)\n",
        "    content = re.sub('[^a-zA-Z]', ' ', content)\n",
        "\n",
        "    content = content.lower()\n",
        "    content = content.split()\n",
        "    content = [lemmatizer.lemmatize(word) for word in content if word not in stopwords.words('english')]\n",
        "    return ' '.join(content)\n",
        "\n",
        "data[\"Message\"] = data[\"Message\"].apply(lemmatizing)\n"
      ],
      "metadata": {
        "id": "rEKMC1I37Mkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separating the data and label\n",
        "X = data['Message'].values\n",
        "y = data['Class'].values"
      ],
      "metadata": {
        "id": "LiYxeelr7Rs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the textual data to Feature vectors\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "vazOaDTf7oxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.fit(X)\n",
        "X = vectorizer.transform(X)"
      ],
      "metadata": {
        "id": "JW8L0I6v7tDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "uHl0W7PQ7vq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'solver': ['liblinear', 'saga'],  # Solver to use\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, scoring='accuracy', cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "print(f\"Best Parameters: {grid.best_params_}\")\n",
        "\n",
        "# Predict and evaluate on the validation set\n",
        "y_pred = best_model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n",
        "accuracy_sklearn = accuracy_score(y_val, y_pred)\n",
        "print(f\"Improved Accuracy using GridSearchCV: {accuracy_sklearn:.2f}\")\n"
      ],
      "metadata": {
        "id": "3RC9LA2h7ymE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Class'] = data['Class'].replace({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Function to predict label for new content\n",
        "def predict_label(text):\n",
        "    # Preprocess the input text\n",
        "    processed_text = lemmatizing(text)\n",
        "    # Transform the text using the fitted vectorizer\n",
        "    text_vector = vectorizer.transform([processed_text])\n",
        "    # Predict the label\n",
        "    predicted_label = best_model.predict(text_vector)\n",
        "    # Map the numerical label to the corresponding emotional label\n",
        "    return label_mapping[predicted_label[0]]\n",
        "\n",
        "# Example of predicting a new input\n",
        "new_input_text = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
        "predicted_label = predict_label(new_input_text)\n",
        "print(f\"Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "cATL9Wnv71Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "with open('best_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(best_model, model_file)\n",
        "\n",
        "# Save the vectorizer\n",
        "with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
        "    pickle.dump(vectorizer, vectorizer_file)\n"
      ],
      "metadata": {
        "id": "zJ3uFaQQ8VH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pvdmVk1j8twf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}